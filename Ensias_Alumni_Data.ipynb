{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3324dcc8",
   "metadata": {},
   "source": [
    "# ENSIAS Alumni LinkedIn Data Scraper\n",
    "\n",
    "This notebook scrapes LinkedIn profiles of ENSIAS alumni to collect professional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ed64d",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b462e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio~=0.30.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: webdriver-manager in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from requests->webdriver-manager) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from requests->webdriver-manager) (2025.8.3)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: lxml in d:\\1 projects\\ensias\\ensias_env\\lib\\site-packages (6.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5abf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd324fc6",
   "metadata": {},
   "source": [
    "## 2. Configure LinkedIn Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a29085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for login to complete...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to LinkedIn login page\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "time.sleep(3)  # Give page time to fully load\n",
    "\n",
    "# Set LinkedIn credentials\n",
    "user_name = 'rachidben460@gmail.com'  # LinkedIn email\n",
    "password = 'mee3sessa1996LEPRO'  # LinkedIn password\n",
    "\n",
    "# Enter credentials and login\n",
    "username_field = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"username\"))\n",
    ")\n",
    "username_field.clear()\n",
    "username_field.send_keys(user_name)\n",
    "\n",
    "password_field = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"password\"))\n",
    ")\n",
    "password_field.clear()\n",
    "password_field.send_keys(password)\n",
    "\n",
    "# Use the correct XPath for the sign-in button\n",
    "login_button = driver.find_element('xpath', '//*[@id=\"organic-div\"]/form/div[4]/button')\n",
    "login_button.click()\n",
    "\n",
    "# Wait for login to complete\n",
    "print(\"Waiting for login to complete...\")\n",
    "time.sleep(8)  # Wait time to ensure page loads completely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a1aa2",
   "metadata": {},
   "source": [
    "## 3. Retrieve Alumni Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75658b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to ENSIAS alumni page (filtered for years 2010-2019)\n",
    "driver.get(\"https://www.linkedin.com/school/ecole-nationale-superieure-d-informatique-et-d-analyse-des-systemes/people/?educationEndYear=2019&educationStartYear=2010\")\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209c7d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked 'Show more results' (1/15)\n",
      "Clicked 'Show more results' (2/15)\n",
      "Clicked 'Show more results' (3/15)\n",
      "Target of 30 profiles reached\n",
      "Finished loading approximately 36 profiles\n"
     ]
    }
   ],
   "source": [
    "# Scroll and click \"Show more results\" to load more profiles\n",
    "target_profiles = 30  # Default target: collect 20 profiles\n",
    "profiles_loaded = 0\n",
    "max_attempts = 15  # Safety limit for maximum attempts\n",
    "\n",
    "# Initial scroll to load first set of profiles\n",
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "time.sleep(3)\n",
    "\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        # Find and click \"Show more results\" button\n",
    "        show_more_button = driver.find_element('xpath', '//span[text()=\"Show more results\"]/parent::button')\n",
    "        \n",
    "        # Scroll to button to ensure it's visible\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", show_more_button)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Click the button\n",
    "        show_more_button.click()\n",
    "        print(f\"Clicked 'Show more results' ({attempt+1}/{max_attempts})\")\n",
    "        \n",
    "        # Wait for new content to load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Estimate number of profiles loaded (each click adds ~12 profiles)\n",
    "        profiles_loaded += 12\n",
    "        \n",
    "        # Stop if we've loaded enough profiles\n",
    "        if profiles_loaded >= target_profiles:\n",
    "            print(f\"Target of {target_profiles} profiles reached\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not find or click 'Show more results' button: {str(e)}\")\n",
    "        # Try scrolling a bit more and retry\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # If we've made multiple attempts without success, we're probably at the end\n",
    "        if attempt > 2:\n",
    "            print(\"No more 'Show more results' button found. All profiles loaded.\")\n",
    "            break\n",
    "\n",
    "print(f\"Finished loading approximately {profiles_loaded} profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db2e4e",
   "metadata": {},
   "source": [
    "## 4. Extract Profile Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a320a75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 unique LinkedIn profiles\n",
      "\n",
      "Sample of profile links:\n",
      "https://www.linkedin.com/in/mouad-bazzi\n",
      "https://www.linkedin.com/in/anas-al-kouraichi-695bb7178\n",
      "https://www.linkedin.com/in/sqrt-negativeone\n",
      "https://www.linkedin.com/in/zakariyaa-amekhroub-54a896198\n",
      "https://www.linkedin.com/in/ismael-sbihi-061121129\n"
     ]
    }
   ],
   "source": [
    "# Get page source and parse with BeautifulSoup\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "# Find all profile cards based on the structure you provided\n",
    "profile_links = soup.find_all('a', class_=lambda c: c and \"pVgHQJscSHWSyOoSAQIMWsVcRbnRZrJxuo\" in c)\n",
    "\n",
    "# If the above doesn't work, try this alternative approach using partial ID match\n",
    "if not profile_links:\n",
    "    profile_links = soup.find_all('a', id=lambda i: i and i.startswith('org-people-profile-card__profile-image-'))\n",
    "\n",
    "# Create a set to store unique profile URLs\n",
    "unique_profile_links = set()\n",
    "\n",
    "# Extract and filter profile links\n",
    "for link in profile_links:\n",
    "    href = link.get('href')\n",
    "    if href:\n",
    "        # Skip school/organization profiles (URLs containing '/school/' or '/company/')\n",
    "        if '/school/' in href or '/company/' in href:\n",
    "            continue\n",
    "            \n",
    "        # Extract the main profile URL by removing query parameters\n",
    "        if '?' in href:\n",
    "            href = href.split('?')[0]\n",
    "        \n",
    "        # Make sure it's a profile URL (should contain '/in/')\n",
    "        if '/in/' in href:\n",
    "            unique_profile_links.add(href)\n",
    "\n",
    "# Convert set to list\n",
    "unique_profile_links_list = list(unique_profile_links)\n",
    "\n",
    "# Display the number of unique profiles found\n",
    "print(f\"Found {len(unique_profile_links_list)} unique LinkedIn profiles\")\n",
    "\n",
    "# Display first 5 profile links as a sample\n",
    "print(\"\\nSample of profile links:\")\n",
    "for link in unique_profile_links_list[:5]:\n",
    "    print(link)\n",
    "\n",
    "# Create full_profile_urls for the scraping section\n",
    "full_profile_urls = unique_profile_links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee359f4d",
   "metadata": {},
   "source": [
    "## 5. Clean and Process Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d372a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited to 48 profiles for processing\n",
      "\n",
      "Sample of processed URLs:\n",
      "https://www.linkedin.com/in/mouad-bazzi/\n",
      "https://www.linkedin.com/in/anas-al-kouraichi-695bb7178/\n",
      "https://www.linkedin.com/in/sqrt-negativeone/\n",
      "https://www.linkedin.com/in/zakariyaa-amekhroub-54a896198/\n",
      "https://www.linkedin.com/in/ismael-sbihi-061121129/\n"
     ]
    }
   ],
   "source": [
    "# Limit to the first 200 profiles (to reduce processing time)\n",
    "profile_links_limited = unique_profile_links_list[:200]\n",
    "print(f\"Limited to {len(profile_links_limited)} profiles for processing\")\n",
    "\n",
    "# Normalize LinkedIn profile URLs\n",
    "normalized_profile_urls = []\n",
    "\n",
    "for url in profile_links_limited:\n",
    "    # Parse URL\n",
    "    parsed_url = urlparse(url)\n",
    "    # Extract domain and path\n",
    "    profile_url = parsed_url.netloc + parsed_url.path + '/'\n",
    "    # Add to list\n",
    "    normalized_profile_urls.append(profile_url)\n",
    "\n",
    "# Add 'https://' prefix to all URLs\n",
    "full_profile_urls = []\n",
    "for url in normalized_profile_urls:\n",
    "    full_url = \"https://\" + url\n",
    "    full_profile_urls.append(full_url)\n",
    "\n",
    "# Display sample of processed URLs\n",
    "print(\"\\nSample of processed URLs:\")\n",
    "for url in full_profile_urls[:5]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d98eab",
   "metadata": {},
   "source": [
    "## 6. Scrape Individual Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2712e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape 20 profiles...\n",
      "Processing profile 1/20: https://www.linkedin.com/in/mouad-bazzi/\n",
      "  - Found name: Mouad Bazzi\n",
      "  - Found location: Prefecture of Casablanca, Casablanca-Settat, Morocco\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: Software Engineer\n",
      "  - Found company: Attijariwafa bank\n",
      "Completed profile 1/20\n",
      "Processing profile 2/20: https://www.linkedin.com/in/anas-al-kouraichi-695bb7178/\n",
      "  - Found name: anas al-kouraichi\n",
      "  - Found location: Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: FY COMPUTING\n",
      "  - Found company: 3 yrs 8 mos\n",
      "Completed profile 2/20\n",
      "Processing profile 3/20: https://www.linkedin.com/in/sqrt-negativeone/\n",
      "  - Found name: Fakhri Mouad\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2023\n",
      "  - Found position: C Developer\n",
      "  - Found company: Journee\n",
      "Completed profile 3/20\n",
      "Processing profile 4/20: https://www.linkedin.com/in/zakariyaa-amekhroub-54a896198/\n",
      "  - Found name: Zakariyaa Amekhroub\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: Cyber Security Analyst\n",
      "  - Found company: Orange Cyberdefense\n",
      "Completed profile 4/20\n",
      "Processing profile 5/20: https://www.linkedin.com/in/ismael-sbihi-061121129/\n",
      "  - Found name: Ismael Sbihi\n",
      "  - Found location: Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2021\n",
      "  - Found position: Assistant projet\n",
      "  - Found company: DevTech Systems, Inc.\n",
      "Completed profile 5/20\n",
      "Processing profile 6/20: https://www.linkedin.com/in/chadi-elhari/\n",
      "  - Found name: Chadi EL HARI\n",
      "  - Found location: Casablanca, Casablanca-Settat, Morocco\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: Big Data & AI Engineer\n",
      "  - Found company: Attijariwafa bank\n",
      "Completed profile 6/20\n",
      "Processing profile 7/20: https://www.linkedin.com/in/hajarrezzouqi/\n",
      "  - Found name: Hajar Rezzouqi\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2016\n",
      "  - Found position: Business Intelligence Engineer\n",
      "  - Found company: PMP\n",
      "Completed profile 7/20\n",
      "Processing profile 8/20: https://www.linkedin.com/in/achraf-nia-3b1603194/\n",
      "  - Found name: Achraf Nia\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2021\n",
      "  - Found position: HENCEFORTH\n",
      "  - Found company: 4 yrs 7 mos\n",
      "Completed profile 8/20\n",
      "Processing profile 9/20: https://www.linkedin.com/in/el-mehdi-zhar-186205186/\n",
      "  - Found name: EL MEHDI ZHAR\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2021\n",
      "  - Found position: AI Software Engineer\n",
      "  - Found company: ThreatsEye\n",
      "Completed profile 9/20\n",
      "Processing profile 10/20: https://www.linkedin.com/in/oumaima-moufid-075ab1195/\n",
      "  - Found name: Oumaima MOUFID\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2020\n",
      "  - Found position: Amazon Web Services (AWS)\n",
      "  - Found company: Full-time\n",
      "Completed profile 10/20\n",
      "Processing profile 11/20: https://www.linkedin.com/in/elhami7/\n",
      "  - Found name: Ahmed Elhami\n",
      "  - Found location: Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2021\n",
      "  - Found position: Business Intelligence Analyst (SAP BO)\n",
      "  - Found company: Pharma Pilot (part of the Walden Group)\n",
      "Completed profile 11/20\n",
      "Processing profile 12/20: https://www.linkedin.com/in/laaouj/\n",
      "  - Found name: Rajia LAAOUJ\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2011\n",
      "  - Found position: Chef de service Accompagnement Projets Sociaux\n",
      "  - Found company: ADD_Officiel\n",
      "Completed profile 12/20\n",
      "Processing profile 13/20: https://www.linkedin.com/in/abdelghani-ridda/\n",
      "  - Found name: Abdelghani Ridda\n",
      "  - Found location: Casablanca Metropolitan Area\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: Amazon Web Services (AWS)\n",
      "  - Found company: 3 yrs 8 mos\n",
      "Completed profile 13/20\n",
      "Processing profile 14/20: https://www.linkedin.com/in/yassine-amrani-960a55190/\n",
      "  - Found name: Yassine Amrani\n",
      "  - Found location: Casablanca Metropolitan Area\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: Deloitte\n",
      "  - Found company: Full-time\n",
      "Completed profile 14/20\n",
      "Processing profile 15/20: https://www.linkedin.com/in/assalielmehdi/\n",
      "  - Found name: El Mehdi ASSALI\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2019\n",
      "  - Found position: Senior Software Engineer\n",
      "  - Found company: BCG X\n",
      "Completed profile 15/20\n",
      "Processing profile 16/20: https://www.linkedin.com/in/houda-fakhkhari-62416149/\n",
      "  - Found name: Houda Fakhkhari\n",
      "  - Found location: Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2013\n",
      "  - Found position: Lalla Salma Foundation\n",
      "  - Found company: Full-time\n",
      "Completed profile 16/20\n",
      "Processing profile 17/20: https://www.linkedin.com/in/oussamahedda/\n",
      "  - Found name: OUSSAMA HEDDA\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2014\n",
      "  - Found position: Senior cloud engineer\n",
      "  - Found company: Deloitte\n",
      "Completed profile 17/20\n",
      "Processing profile 18/20: https://www.linkedin.com/in/oumayma-gharib/\n",
      "  - Found name: Oumayma GHARIB\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2019\n",
      "  - Found position: Senior Full Stack Engineer\n",
      "  - Found company: Quinten\n",
      "Completed profile 18/20\n",
      "Processing profile 19/20: https://www.linkedin.com/in/adnan-elharti/\n",
      "  - Found name: Adnan Elharti\n",
      "  - Found location: Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2021\n",
      "  - Found position: Developer Backoffice ORACLE\n",
      "  - Found company: Bouygues Telecom\n",
      "Completed profile 19/20\n",
      "Processing profile 20/20: https://www.linkedin.com/in/oussama-aarab/\n",
      "  - Found name: Oussama Aarab\n",
      "  - Found location: Rabat, Rabat-Salé-Kénitra, Morocco\n",
      "  - Found graduation year: 2022\n",
      "  - Found position: SecDojo\n",
      "  - Found company: 2 yrs 9 mos\n",
      "Completed profile 20/20\n",
      "Profile scraping complete\n"
     ]
    }
   ],
   "source": [
    "# Set the number of profiles to scrape (limited to 3 for testing)\n",
    "profiles_to_scrape = 20  # Testing with just 3 profiles\n",
    "\n",
    "# Initialize lists to store extracted data\n",
    "names = []\n",
    "locations = []\n",
    "companies = []\n",
    "positions = []\n",
    "graduation_years = []  # Will now be populated\n",
    "\n",
    "# Counter for progress tracking\n",
    "profile_counter = 0\n",
    "# Limit the profiles to scrape to the specified number\n",
    "profiles_to_process = full_profile_urls[:profiles_to_scrape]\n",
    "total_profiles = len(profiles_to_process)\n",
    "\n",
    "print(f\"Starting to scrape {total_profiles} profiles...\")\n",
    "\n",
    "# Process each profile\n",
    "for profile_url in profiles_to_process:\n",
    "    profile_counter += 1\n",
    "    print(f\"Processing profile {profile_counter}/{total_profiles}: {profile_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Visit profile page\n",
    "        driver.get(profile_url)\n",
    "        time.sleep(8)  # Increased wait time for page to fully load\n",
    "        \n",
    "        # Extract name from title\n",
    "        name = \"\"\n",
    "        try:\n",
    "            title_element = driver.find_element('xpath', '/html/head/title')\n",
    "            if title_element:\n",
    "                full_title = title_element.get_attribute('textContent')\n",
    "                # Extract name from title format \"(32) NAME | LinkedIn\"\n",
    "                if \"|\" in full_title:\n",
    "                    name_part = full_title.split(\"|\")[0].strip()\n",
    "                    # Remove any leading numbers/parentheses\n",
    "                    if \")\" in name_part:\n",
    "                        name = name_part.split(\")\")[1].strip()\n",
    "                    else:\n",
    "                        name = name_part\n",
    "                names.append(name)\n",
    "                print(f\"  - Found name: {name}\")\n",
    "            else:\n",
    "                # Try alternative method\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'lxml')\n",
    "                name_element = soup.find('h1', {'class': 'text-heading-xlarge'})\n",
    "                if name_element:\n",
    "                    name = name_element.get_text().strip()\n",
    "                    names.append(name)\n",
    "                    print(f\"  - Found name: {name}\")\n",
    "                else:\n",
    "                    names.append(\"\")\n",
    "                    print(\"  - Name not found\")\n",
    "        except Exception as e:\n",
    "            names.append(\"\")\n",
    "            print(f\"  - Error extracting name: {str(e)}\")\n",
    "        \n",
    "        # Extract location\n",
    "        location = \"\"\n",
    "        try:\n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            \n",
    "            # Try multiple location selectors\n",
    "            location_element = soup.find('span', {'class': 'text-body-small inline t-black--light break-words'})\n",
    "            if not location_element:\n",
    "                location_xpath = '//*[@id=\"profile-content\"]/div/div[2]/div/div/main/section[1]/div[2]/div[2]/div[2]/span[1]'\n",
    "                location_element = driver.find_element('xpath', location_xpath)\n",
    "                if location_element:\n",
    "                    location = location_element.text.strip()\n",
    "            else:\n",
    "                location = location_element.get_text().strip()\n",
    "                \n",
    "            if location:\n",
    "                locations.append(location)\n",
    "                print(f\"  - Found location: {location}\")\n",
    "            else:\n",
    "                locations.append(\"\")\n",
    "                print(\"  - Location not found\")\n",
    "        except Exception as e:\n",
    "            locations.append(\"\")\n",
    "            print(f\"  - Error extracting location: {str(e)}\")\n",
    "        \n",
    "        # Extract graduation year from education section\n",
    "        graduation_year = \"\"\n",
    "        try:\n",
    "            # Navigate to education section directly\n",
    "            education_url = profile_url + \"details/education/\"\n",
    "            driver.get(education_url)\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            \n",
    "            # Try to find ENSIAS in the education list\n",
    "            ensias_found = False\n",
    "            \n",
    "            # Look for education institutions\n",
    "            education_elements = soup.find_all('div', class_=lambda c: c and 't-bold' in c)\n",
    "            \n",
    "            for i, edu_element in enumerate(education_elements):\n",
    "                edu_span = edu_element.find('span', {'aria-hidden': 'true'})\n",
    "                if edu_span:\n",
    "                    edu_text = edu_span.get_text().strip()\n",
    "                    # Check if this is ENSIAS\n",
    "                    if \"ENSIAS\" in edu_text or \"Ecole Nationale Supérieure d'Informatique\" in edu_text:\n",
    "                        ensias_found = True\n",
    "                        # Look for the date span associated with this education entry\n",
    "                        # It's typically in a t-black--light class span near this element\n",
    "                        \n",
    "                        # First, try to find the parent li element\n",
    "                        parent_li = edu_element\n",
    "                        while parent_li and parent_li.name != 'li':\n",
    "                            parent_li = parent_li.parent\n",
    "                        \n",
    "                        if parent_li:\n",
    "                            # Find the date span within this li\n",
    "                            date_span = parent_li.find('span', class_=lambda c: c and 't-black--light' in c)\n",
    "                            if date_span:\n",
    "                                date_inner_span = date_span.find('span', {'aria-hidden': 'true'})\n",
    "                                if date_inner_span:\n",
    "                                    date_text = date_inner_span.get_text().strip()\n",
    "                                    # Extract the graduation year from the date range\n",
    "                                    # Format could be \"2018 - 2021\" or similar\n",
    "                                    if \"-\" in date_text:\n",
    "                                        graduation_year = date_text.split(\"-\")[1].strip()\n",
    "                                        # Clean up any non-numeric characters\n",
    "                                        import re\n",
    "                                        graduation_year = re.sub(r'[^0-9]', '', graduation_year)\n",
    "                                        # Take just the first 4 digits (the year)\n",
    "                                        if len(graduation_year) >= 4:\n",
    "                                            graduation_year = graduation_year[:4]\n",
    "                                    else:\n",
    "                                        # If there's no range, just try to extract any 4-digit year\n",
    "                                        import re\n",
    "                                        years = re.findall(r'\\b(20\\d{2})\\b', date_text)\n",
    "                                        if years:\n",
    "                                            graduation_year = years[-1]  # Take the last year found\n",
    "            \n",
    "            # If we didn't find ENSIAS specifically, but found some education entries,\n",
    "            # take the most recent graduation year as a fallback\n",
    "            if not ensias_found and education_elements:\n",
    "                date_spans = soup.find_all('span', class_=lambda c: c and 't-black--light' in c)\n",
    "                for date_span in date_spans:\n",
    "                    inner_span = date_span.find('span', {'aria-hidden': 'true'})\n",
    "                    if inner_span:\n",
    "                        date_text = inner_span.get_text().strip()\n",
    "                        if \"-\" in date_text:\n",
    "                            year_part = date_text.split(\"-\")[1].strip()\n",
    "                            import re\n",
    "                            year_digits = re.sub(r'[^0-9]', '', year_part)\n",
    "                            if len(year_digits) >= 4:\n",
    "                                graduation_year = year_digits[:4]\n",
    "                                break\n",
    "                        else:\n",
    "                            import re\n",
    "                            years = re.findall(r'\\b(20\\d{2})\\b', date_text)\n",
    "                            if years:\n",
    "                                graduation_year = years[-1]\n",
    "                                break\n",
    "            \n",
    "            if graduation_year:\n",
    "                graduation_years.append(graduation_year)\n",
    "                print(f\"  - Found graduation year: {graduation_year}\")\n",
    "            else:\n",
    "                graduation_years.append(\"\")\n",
    "                print(\"  - Graduation year not found\")\n",
    "        except Exception as e:\n",
    "            graduation_years.append(\"\")\n",
    "            print(f\"  - Error extracting graduation year: {str(e)}\")\n",
    "        \n",
    "        # Extract current position and company - try experience section directly\n",
    "        position = \"\"\n",
    "        company = \"\"\n",
    "        try:\n",
    "            # Navigate to experience section directly\n",
    "            experience_url = profile_url + \"details/experience/\"\n",
    "            driver.get(experience_url)\n",
    "            time.sleep(5)  # Wait for page to load\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            \n",
    "            # Find position using the class from your working code\n",
    "            position_element = soup.find('div', class_=lambda c: c and 't-bold' in c)\n",
    "            if position_element:\n",
    "                position_span = position_element.find('span', {'aria-hidden': 'true'})\n",
    "                if position_span:\n",
    "                    position = position_span.get_text().strip()\n",
    "            \n",
    "            # Find company using the class from your working code\n",
    "            company_element = soup.find('span', class_='t-14 t-normal')\n",
    "            if company_element:\n",
    "                company_span = company_element.find('span', {'aria-hidden': 'true'})\n",
    "                if company_span:\n",
    "                    company_text = company_span.get_text().strip()\n",
    "                    # Split by \"·\" as in your working code\n",
    "                    company_parts = company_text.split(\"·\")\n",
    "                    if len(company_parts) > 0:\n",
    "                        company = company_parts[0].strip()\n",
    "            \n",
    "            if not position or not company:\n",
    "                # If not found, go back to main profile and try the original method\n",
    "                driver.get(profile_url)\n",
    "                time.sleep(5)\n",
    "                \n",
    "                # Scroll down to ensure experience section loads\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'lxml')\n",
    "                \n",
    "                # Find the experience section container\n",
    "                experience_container = soup.find('div', class_='pvs-list__container')\n",
    "                \n",
    "                if experience_container:\n",
    "                    # Find the first (most recent) experience item\n",
    "                    first_experience = experience_container.find('li', class_='pvs-list__paged-list-item')\n",
    "                    \n",
    "                    if first_experience:\n",
    "                        # Extract position\n",
    "                        if not position:\n",
    "                            position_div = first_experience.find('div', class_=lambda c: c and 't-bold' in c)\n",
    "                            if position_div:\n",
    "                                position_span = position_div.find('span', {'aria-hidden': 'true'})\n",
    "                                if position_span:\n",
    "                                    position = position_span.get_text(strip=True)\n",
    "                        \n",
    "                        # Extract company\n",
    "                        if not company:\n",
    "                            company_spans = first_experience.find_all('span', class_=lambda c: c and 't-14' in c and 't-normal' in c)\n",
    "                            for span in company_spans:\n",
    "                                if 't-black--light' not in span.get('class', []):\n",
    "                                    company_span = span.find('span', {'aria-hidden': 'true'})\n",
    "                                    if company_span:\n",
    "                                        company_text = company_span.get_text(strip=True)\n",
    "                                        if \"·\" in company_text:\n",
    "                                            company = company_text.split(\"·\")[0].strip()\n",
    "                                        else:\n",
    "                                            company = company_text\n",
    "                                        break\n",
    "            \n",
    "            positions.append(position)\n",
    "            companies.append(company)\n",
    "            \n",
    "            if position:\n",
    "                print(f\"  - Found position: {position}\")\n",
    "            else:\n",
    "                print(\"  - Position not found\")\n",
    "                \n",
    "            if company:\n",
    "                print(f\"  - Found company: {company}\")\n",
    "            else:\n",
    "                print(\"  - Company not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            positions.append(\"\")\n",
    "            companies.append(\"\")\n",
    "            print(f\"  - Error extracting position/company: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  - Error processing profile: {str(e)}\")\n",
    "        # Append empty values for failed profiles\n",
    "        if len(names) < profile_counter:\n",
    "            names.append(\"\")\n",
    "        if len(locations) < profile_counter:\n",
    "            locations.append(\"\")\n",
    "        if len(companies) < profile_counter:\n",
    "            companies.append(\"\")\n",
    "        if len(positions) < profile_counter:\n",
    "            positions.append(\"\")\n",
    "        if len(graduation_years) < profile_counter:\n",
    "            graduation_years.append(\"\")\n",
    "    \n",
    "    # Print progress for each profile\n",
    "    print(f\"Completed profile {profile_counter}/{total_profiles}\")\n",
    "\n",
    "print(\"Profile scraping complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05855f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection summary:\n",
      "Names: 20\n",
      "Locations: 20\n",
      "Graduation years: 20\n",
      "Positions: 20\n",
      "Companies: 20\n"
     ]
    }
   ],
   "source": [
    "# Display counts for collected data\n",
    "print(\"Data collection summary:\")\n",
    "print(f\"Names: {len(names)}\")\n",
    "print(f\"Locations: {len(locations)}\")\n",
    "print(f\"Graduation years: {len(graduation_years)}\")\n",
    "print(f\"Positions: {len(positions)}\")\n",
    "print(f\"Companies: {len(companies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3d92e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to D:\\1 Projects\\Ensias\\Data\\ensias_alumni_profiles.csv\n",
      "DataFrame shape: (20, 5)\n",
      "\n",
      "Preview of saved data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Graduation_Year</th>\n",
       "      <th>Position</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mouad Bazzi</td>\n",
       "      <td>Prefecture of Casablanca, Casablanca-Settat, M...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Attijariwafa bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anas al-kouraichi</td>\n",
       "      <td>Rabat-Salé-Kénitra, Morocco</td>\n",
       "      <td>2022</td>\n",
       "      <td>FY COMPUTING</td>\n",
       "      <td>3 yrs 8 mos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fakhri Mouad</td>\n",
       "      <td>Rabat, Rabat-Salé-Kénitra, Morocco</td>\n",
       "      <td>2023</td>\n",
       "      <td>C Developer</td>\n",
       "      <td>Journee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zakariyaa Amekhroub</td>\n",
       "      <td>Rabat, Rabat-Salé-Kénitra, Morocco</td>\n",
       "      <td>2022</td>\n",
       "      <td>Cyber Security Analyst</td>\n",
       "      <td>Orange Cyberdefense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ismael Sbihi</td>\n",
       "      <td>Rabat-Salé-Kénitra, Morocco</td>\n",
       "      <td>2021</td>\n",
       "      <td>Assistant projet</td>\n",
       "      <td>DevTech Systems, Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name                                           Location  \\\n",
       "0          Mouad Bazzi  Prefecture of Casablanca, Casablanca-Settat, M...   \n",
       "1    anas al-kouraichi                        Rabat-Salé-Kénitra, Morocco   \n",
       "2         Fakhri Mouad                 Rabat, Rabat-Salé-Kénitra, Morocco   \n",
       "3  Zakariyaa Amekhroub                 Rabat, Rabat-Salé-Kénitra, Morocco   \n",
       "4         Ismael Sbihi                        Rabat-Salé-Kénitra, Morocco   \n",
       "\n",
       "  Graduation_Year                Position                Company  \n",
       "0            2022       Software Engineer      Attijariwafa bank  \n",
       "1            2022            FY COMPUTING            3 yrs 8 mos  \n",
       "2            2023             C Developer                Journee  \n",
       "3            2022  Cyber Security Analyst    Orange Cyberdefense  \n",
       "4            2021        Assistant projet  DevTech Systems, Inc.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the collected data to a CSV file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = {\n",
    "    'Name': names,\n",
    "    'Location': locations,\n",
    "    'Graduation_Year': graduation_years,\n",
    "    'Position': positions,\n",
    "    'Company': companies\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV in the specified directory\n",
    "output_file = os.path.join(\"D:\\\\1 Projects\\\\Ensias\\\\Data\", \"ensias_alumni_profiles.csv\")\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')  # utf-8-sig preserves special characters\n",
    "\n",
    "print(f\"Data saved successfully to {output_file}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Display the first few rows to verify the content\n",
    "print(\"\\nPreview of saved data:\")\n",
    "display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ensias_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
